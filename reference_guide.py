
"""
Reference guide for Distillation and Quantization methods.
"""

DISTILLATION_METHODS = """
ğŸ§  Knowledge Distillation â€” Complete Method Map
ğŸ”¹ 1. Classic Distillation
â€¢ 	Logit Distillation (Soft Targets)
â€¢ 	Hardâ€‘Label Distillation
â€¢ 	Temperatureâ€‘Scaled Distillation
ğŸ”¹ 2. Featureâ€‘Based Distillation
â€¢ 	Intermediate Feature Matching
â€¢ 	Attention Map Distillation
â€¢ 	Activation/Representation Distillation
â€¢ 	Hintâ€‘based Distillation (FitNets)
â€¢ 	Neuron Selectivity Transfer (NST)
â€¢ 	Similarityâ€‘Preserving KD (SPKD)
â€¢ 	Correlation Congruence KD (CCKD)
â€¢ 	Relational KD (RKD)
â€¢ 	Contrastive Representation KD
ğŸ”¹ 3. Lossâ€‘Functionâ€‘Driven Distillation
â€¢ 	KLDâ€‘based KD
â€¢ 	MSE Feature Loss
â€¢ 	Cosine Similarity Loss
â€¢ 	Triplet Loss KD
â€¢ 	Marginâ€‘based KD
â€¢ 	Mutual Information KD
ğŸ”¹ 4. Multiâ€‘Teacher Distillation
â€¢ 	Ensemble Teacher KD
â€¢ 	Weighted Multiâ€‘Teacher KD
â€¢ 	Gated/Adaptive Teacher KD
â€¢ 	Mixtureâ€‘ofâ€‘Experts KD
â€¢ 	Crossâ€‘Teacher Consistency KD
ğŸ”¹ 5. Selfâ€‘Distillation
â€¢ 	Bornâ€‘Again Networks (BAN)
â€¢ 	Deep Mutual Learning (DML)
â€¢ 	Online KD / Selfâ€‘Training
â€¢ 	Layerâ€‘toâ€‘Layer Selfâ€‘Distillation
â€¢ 	Progressive Selfâ€‘Distillation
ğŸ”¹ 6. Taskâ€‘Specific Distillation
â€¢ 	Sequenceâ€‘Level KD (NLP)
â€¢ 	Tokenâ€‘Level KD (Transformers)
â€¢ 	Layerâ€‘Drop KD
â€¢ 	Responseâ€‘Based KD (LLMs)
â€¢ 	RLâ€‘KD (Policy Distillation)
â€¢ 	Diffusionâ€‘Model KD (Score Distillation, Consistency Distillation)
â€¢ 	Visionâ€‘Transformer KD (ViTâ€‘specific)
ğŸ”¹ 7. Dataâ€‘Centric Distillation
â€¢ 	Data Augmentation KD
â€¢ 	Noisy Student Training
â€¢ 	Pseudoâ€‘Label KD
â€¢ 	Curriculum KD
â€¢ 	Dataset Distillation (Synthetic Data KD)
ğŸ”¹ 8. Architectureâ€‘Aware Distillation
â€¢ 	Crossâ€‘Architecture KD (CNNâ†’Transformer, etc.)
â€¢ 	Width/Depthâ€‘Reduced KD
â€¢ 	Projectionâ€‘Layer KD
â€¢ 	Teacherâ€‘Student Alignment KD

ğŸ§  Complete List of Distillation Methods (Full Taxonomy)
ğŸ”· 1. Logitâ€‘Level Distillation (Responseâ€‘Based)
â€¢ 	Softâ€‘Target Distillation
â€¢ 	Hardâ€‘Label Distillation
â€¢ 	Temperatureâ€‘Scaled KD
â€¢ 	Kullbackâ€“Leibler KD
â€¢ 	Crossâ€‘Entropy KD
â€¢ 	Labelâ€‘Smoothing KD
â€¢ 	Confidenceâ€‘Penalty KD
â€¢ 	Dark Knowledge Distillation
â€¢ 	Responseâ€‘Consistency KD
â€¢ 	Multiâ€‘Teacher Logit Averaging
â€¢ 	Gated Logit Fusion KD
â€¢ 	Mixtureâ€‘ofâ€‘Experts Logit KD

ğŸ”· 2. Featureâ€‘Level Distillation (Intermediate Representations)
â€¢ 	Feature Map Matching
â€¢ 	Activation Matching
â€¢ 	Hiddenâ€‘State Distillation
â€¢ 	Attention Map Distillation
â€¢ 	Transformer Attention Head Distillation
â€¢ 	FitNets (Hintâ€‘Based Distillation)
â€¢ 	Neuron Selectivity Transfer (NST)
â€¢ 	Similarityâ€‘Preserving KD (SPKD)
â€¢ 	Correlation Congruence KD (CCKD)
â€¢ 	Relational KD (RKD)
â€¢ 	Distanceâ€‘Wise RKD
â€¢ 	Angleâ€‘Wise RKD
â€¢ 	Contrastive Representation KD
â€¢ 	Gram Matrix Distillation
â€¢ 	Jacobian Matching
â€¢ 	Layerâ€‘toâ€‘Layer Projection KD
â€¢ 	Crossâ€‘Architecture Feature Alignment

ğŸ”· 3. Relationâ€‘Based Distillation
â€¢ 	Pairwise Relation KD
â€¢ 	Triplet Relation KD
â€¢ 	Structural Relation KD
â€¢ 	Graphâ€‘Based KD
â€¢ 	Relational Knowledge Transfer (RKT)
â€¢ 	Instanceâ€‘Relation KD
â€¢ 	Classâ€‘Relation KD

ğŸ”· 4. Selfâ€‘Distillation
â€¢ 	Bornâ€‘Again Networks (BAN)
â€¢ 	Deep Mutual Learning (DML)
â€¢ 	Online Selfâ€‘Distillation
â€¢ 	Layerâ€‘toâ€‘Layer Selfâ€‘Distillation
â€¢ 	Progressive Selfâ€‘Distillation
â€¢ 	Snapshot Distillation
â€¢ 	Temporal Ensembling KD
â€¢ 	EMAâ€‘Teacher KD (Mean Teacher)
â€¢ 	Selfâ€‘Training with Pseudoâ€‘Labels

ğŸ”· 5. Multiâ€‘Teacher Distillation
â€¢ 	Ensemble Teacher KD
â€¢ 	Weighted Multiâ€‘Teacher KD
â€¢ 	Adaptive/Gated Teacher KD
â€¢ 	Mixtureâ€‘ofâ€‘Experts KD
â€¢ 	Crossâ€‘Teacher Consistency KD
â€¢ 	Teacherâ€‘Routing KD
â€¢ 	Teacherâ€‘Student Graph KD

ğŸ”· 6. Taskâ€‘Specific Distillation
NLP / LLMs
â€¢ 	Sequenceâ€‘Level KD
â€¢ 	Tokenâ€‘Level KD
â€¢ 	Hiddenâ€‘State KD
â€¢ 	Attentionâ€‘Pattern KD
â€¢ 	Responseâ€‘Style KD
â€¢ 	Instructionâ€‘Following KD
â€¢ 	RLHFâ€‘toâ€‘SL KD (Supervised Distillation of RLHF Models)
â€¢ 	Preferenceâ€‘Model Distillation
â€¢ 	Chainâ€‘ofâ€‘Thought Distillation
â€¢ 	Selfâ€‘Consistency Distillation
â€¢ 	Logitâ€‘Bias Distillation
â€¢ 	KVâ€‘Cache Distillation
Vision
â€¢ 	Feature Pyramid KD
â€¢ 	Objectâ€‘Detection KD
â€¢ 	Semantic Segmentation KD
â€¢ 	Pose Estimation KD
â€¢ 	Heatmap Distillation
â€¢ 	Regionâ€‘Proposal KD
Diffusion Models
â€¢ 	Score Distillation
â€¢ 	Score Distillation Sampling (SDS)
â€¢ 	Consistency Distillation
â€¢ 	Latentâ€‘Space Distillation
â€¢ 	Noiseâ€‘Prediction Distillation
â€¢ 	Denoiserâ€‘toâ€‘UNet Distillation
â€¢ 	Teacherâ€‘Free Guidance Distillation
Reinforcement Learning
â€¢ 	Policy Distillation
â€¢ 	Valueâ€‘Function Distillation
â€¢ 	Qâ€‘Function Distillation
â€¢ 	Behavior Cloning KD
â€¢ 	Trajectoryâ€‘Level KD
â€¢ 	Advantageâ€‘Weighted KD

ğŸ”· 7. Dataâ€‘Centric Distillation
â€¢ 	Noisy Student Training
â€¢ 	Pseudoâ€‘Label Distillation
â€¢ 	Curriculum Distillation
â€¢ 	Data Augmentation KD
â€¢ 	Hardâ€‘Example KD
â€¢ 	Softâ€‘Example KD
â€¢ 	Dataset Distillation (Synthetic Data KD)
â€¢ 	Metaâ€‘Learning KD
â€¢ 	Knowledge Transfer via Synthetic Gradients

ğŸ”· 8. Architectureâ€‘Aware Distillation
â€¢ 	Crossâ€‘Architecture KD (CNNâ†’Transformer, ViTâ†’CNN, etc.)
â€¢ 	Widthâ€‘Reduced KD
â€¢ 	Depthâ€‘Reduced KD
â€¢ 	Projectionâ€‘Layer KD
â€¢ 	Bottleneck KD
â€¢ 	Sparseâ€‘toâ€‘Dense KD
â€¢ 	Denseâ€‘toâ€‘Sparse KD
â€¢ 	Quantizationâ€‘Aware KD (KDâ€‘QAT)
â€¢ 	Pruningâ€‘Aware KD

ğŸ”· 9. Modalityâ€‘Specific Distillation
Vision â†’ Language
â€¢ 	CLIP Distillation
â€¢ 	Visionâ€‘Language Alignment KD
â€¢ 	Crossâ€‘Modal Embedding KD
Audio
â€¢ 	Spectrogram KD
â€¢ 	Waveform KD
â€¢ 	Phonemeâ€‘Level KD
Multimodal
â€¢ 	Crossâ€‘Modal Consistency KD
â€¢ 	Joint Embedding KD
â€¢ 	Fusionâ€‘Layer KD

ğŸ”· 10. Optimizationâ€‘Driven Distillation
â€¢ 	Adversarial Distillation (GANâ€‘based KD)
â€¢ 	Contrastive KD
â€¢ 	Marginâ€‘Based KD
â€¢ 	Mutual Information KD
â€¢ 	Entropyâ€‘Regularized KD
â€¢ 	Teacherâ€‘Student Adversarial Alignment
â€¢ 	Optimal Transport KD

ğŸ”· 11. Hybrid Distillation Methods
â€¢ 	KD + QAT (Quantizationâ€‘Aware Distillation)
â€¢ 	KD + PTQ (Teacherâ€‘Guided Calibration)
â€¢ 	KD + Pruning
â€¢ 	KD + Lowâ€‘Rank Factorization
â€¢ 	KD + MoE Routing
â€¢ 	KD + Synthetic Data Generation
â€¢ 	KD + Reinforcement Learning
â€¢ 	KD + Consistency Models
"""

QUANTIZATION_METHODS = """
âš™ï¸ Quantization â€” Complete Method Map
ğŸ”¹ 1. Postâ€‘Training Quantization (PTQ)
â€¢ 	PTQâ€‘Dynamic
â€¢ 	PTQâ€‘Static (Calibrationâ€‘Based)
â€¢ 	PTQâ€‘Integer (INT8)
â€¢ 	PTQâ€‘FP16 / BF16
â€¢ 	PTQâ€‘INT4
â€¢ 	PTQâ€‘INT2 / Binary / Ternary
â€¢ 	GPTQ (Gradientâ€‘based PTQ)
â€¢ 	AWQ (Activationâ€‘Aware Weight Quantization)
â€¢ 	ZeroQuant
â€¢ 	SmoothQuant
â€¢ 	RPTQ (Roundâ€‘toâ€‘Nearestâ€‘Powerâ€‘ofâ€‘Two)
ğŸ”¹ 2. Quantizationâ€‘Aware Training (QAT)
â€¢ 	Fakeâ€‘Quantization QAT
â€¢ 	LSQ (Learned Step Size Quantization)
â€¢ 	LSQ+
â€¢ 	PACT (Parameterized Clipping Activation)
â€¢ 	DoReFaâ€‘Net
â€¢ 	QATâ€‘INT8
â€¢ 	QATâ€‘INT4
â€¢ 	QATâ€‘Binary / Ternary Networks
ğŸ”¹ 3. Mixedâ€‘Precision Quantization
â€¢ 	Layerâ€‘Wise Mixed Precision
â€¢ 	Channelâ€‘Wise Mixed Precision
â€¢ 	Tokenâ€‘Wise Mixed Precision (LLMs)
â€¢ 	Hardwareâ€‘Aware Mixed Precision
â€¢ 	AutoMLâ€‘Driven Precision Search
ğŸ”¹ 4. Structured Quantization
â€¢ 	Blockwise Quantization (e.g., 32Ã—32 blocks)
â€¢ 	Groupwise Quantization
â€¢ 	Row/Column Quantization
â€¢ 	Tensorâ€‘RT Style Perâ€‘Channel Quantization
ğŸ”¹ 5. Vector & Codebook Quantization
â€¢ 	Product Quantization (PQ)
â€¢ 	Residual Quantization (RQ)
â€¢ 	Additive Quantization (AQ)
â€¢ 	VQâ€‘VAE Quantization
â€¢ 	Codebookâ€‘Based Weight Sharing
ğŸ”¹ 6. LLMâ€‘Specific Quantization
â€¢ 	GPTQ
â€¢ 	AWQ
â€¢ 	SmoothQuant
â€¢ 	ZeroQuant
â€¢ 	LLM.int8()
â€¢ 	LLM.int4()
â€¢ 	Activationâ€‘Outlier Suppression Quantization
â€¢ 	KVâ€‘Cache Quantization
â€¢ 	Groupwise Quantization for Attention Blocks

ğŸ§© Bonus: Hybrid Methods (Distillation + Quantization)
These are increasingly common in production pipelines:
â€¢ 	KDâ€‘QAT (Distillationâ€‘Guided Quantizationâ€‘Aware Training)
â€¢ 	KDâ€‘PTQ (Teacherâ€‘Guided Calibration)
â€¢ 	Featureâ€‘Aligned QAT
â€¢ 	Logitâ€‘Aligned QAT
â€¢ 	Consistencyâ€‘Distilled Quantization (Diffusion/LLMs)
â€¢ 	Multiâ€‘Teacher Quantization Guidance

ğŸ”¢ All Quantization Formats (Complete List)
ğŸŸ¦ 8â€‘bit Formats
â€¢ 	INT8
â€¢ 	UINT8
â€¢ 	FP8â€‘E4M3
â€¢ 	FP8â€‘E5M2
â€¢ 	NF8 (Normalâ€‘Floatâ€‘8)
â€¢ 	MXFP8 (Microsoft FP8 variant)
â€¢ 	INT8â€‘perâ€‘tensor
â€¢ 	INT8â€‘perâ€‘channel
â€¢ 	INT8â€‘perâ€‘group
â€¢ 	INT8â€‘asymmetric
â€¢ 	INT8â€‘symmetric
â€¢ 	INT8â€‘dynamic
â€¢ 	INT8â€‘static (calibrated)
â€¢ 	LLM.int8()
â€¢ 	INT8â€‘KV cache quantization

ğŸŸ© 6â€‘bit Formats
â€¢ 	INT6
â€¢ 	UINT6
â€¢ 	NF6
â€¢ 	GPTQâ€‘INT6
â€¢ 	Groupwise INT6
â€¢ 	Activationâ€‘aware INT6

ğŸŸ§ 5â€‘bit Formats
â€¢ 	INT5
â€¢ 	UINT5
â€¢ 	NF5
â€¢ 	LLM.int5()
â€¢ 	Groupwise INT5
â€¢ 	Codebook 5â€‘bit (VQ)

ğŸŸ¥ 4â€‘bit Formats
â€¢ 	INT4
â€¢ 	UINT4
â€¢ 	NF4 (NormalFloatâ€‘4)
â€¢ 	FP4
â€¢ 	FP4â€‘E2M1
â€¢ 	FP4â€‘E3M0
â€¢ 	QLoRA NF4
â€¢ 	QLoRA FP4
â€¢ 	GPTQâ€‘INT4
â€¢ 	AWQâ€‘INT4
â€¢ 	ZeroQuantâ€‘INT4
â€¢ 	SmoothQuantâ€‘INT4
â€¢ 	INT4â€‘perâ€‘channel
â€¢ 	INT4â€‘perâ€‘group
â€¢ 	INT4â€‘KV cache quantization
â€¢ 	Ternaryâ€‘4 hybrid (2â€‘bit weights + 2â€‘bit scaling)

ğŸŸª 3â€‘bit Formats
â€¢ 	INT3
â€¢ 	UINT3
â€¢ 	NF3
â€¢ 	Ternaryâ€‘plusâ€‘scale (3â€‘bit effective)
â€¢ 	Groupwise INT3
â€¢ 	Codebook 3â€‘bit (PQ/RQ/AQ)

âš« 2â€‘bit Formats
â€¢ 	INT2
â€¢ 	UINT2
â€¢ 	Binaryâ€‘2 hybrid
â€¢ 	Ternary (âˆ’1, 0, +1)
â€¢ 	DoReFa 2â€‘bit
â€¢ 	XNORâ€‘Net 2â€‘bit
â€¢ 	Groupwise INT2
â€¢ 	Codebook 2â€‘bit

âšª 1â€‘bit Formats
â€¢ 	Binary (âˆ’1, +1)
â€¢ 	XNORâ€‘Binary
â€¢ 	Binaryâ€‘Weight Networks (BWN)
â€¢ 	Binaryâ€‘Activation Networks
â€¢ 	XNORâ€‘Net
â€¢ 	Bitâ€‘packing formats (hardwareâ€‘specific)

ğŸ§© Floatingâ€‘Point Lowâ€‘Precision Formats
These are used heavily in NVIDIA Hopper, AMD MI300, and TPU v5e:
â€¢ 	FP16
â€¢ 	BF16
â€¢ 	FP8â€‘E4M3
â€¢ 	FP8â€‘E5M2
â€¢ 	FP6 (experimental)
â€¢ 	FP4
â€¢ 	FP3 (research)
â€¢ 	Hybrid FP8/INT8
â€¢ 	Hybrid FP8/INT4

ğŸ§± Structured / Block Formats
These arenâ€™t bitâ€‘widths but quantization layouts:
â€¢ 	Blockwise INT8 (e.g., 32Ã—32)
â€¢ 	Blockwise INT4
â€¢ 	Groupwise INT4/INT8
â€¢ 	Rowâ€‘wise quantization
â€¢ 	Columnâ€‘wise quantization
â€¢ 	Tensorâ€‘RT perâ€‘channel formats
â€¢ 	KVâ€‘cache block quantization
â€¢ 	Activationâ€‘outlier suppression formats

ğŸ§¬ Vector / Codebook Formats
Used in VQâ€‘VAE, PQ, RQ, and LLM compression:
â€¢ 	Product Quantization (PQ)
â€¢ 	Residual Quantization (RQ)
â€¢ 	Additive Quantization (AQ)
â€¢ 	Codebook 8â€‘bit
â€¢ 	Codebook 4â€‘bit
â€¢ 	Codebook 3â€‘bit
â€¢ 	Codebook 2â€‘bit
â€¢ 	VQâ€‘VAE discrete latent codes

ğŸŸ¨ LLMâ€‘Specific Formats (Complete)
â€¢ 	GPTQ (INT3/4/6)
â€¢ 	AWQ (INT4/INT8)
â€¢ 	SmoothQuant (INT8/INT4)
â€¢ 	ZeroQuant (INT8/INT4)
â€¢ 	QLoRA NF4
â€¢ 	QLoRA FP4
â€¢ 	LLM.int8()
â€¢ 	LLM.int4()
â€¢ 	Activationâ€‘aware INT8
â€¢ 	KVâ€‘cache quantization (INT8/INT4/FP8)
â€¢ 	Groupwise quantization for attention blocks
"""
